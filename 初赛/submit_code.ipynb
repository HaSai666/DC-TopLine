{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入相关库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection  import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import datetime\n",
    "from tqdm import trange\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import warnings\n",
    "import time\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "from xgboost import *\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.linear_model import  *\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sympy import *\n",
    "from xgboost import plot_importance\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict={}\n",
    "dict['四川省']=32617\n",
    "dict['安徽省']=32001\n",
    "dict['福建省']=58145\n",
    "dict['河北省']=38909\n",
    "dict['贵州省']=23151\n",
    "dict['山东省']=56885\n",
    "dict['黑龙江省']=37697\n",
    "dict['广东省']=58833\n",
    "dict['北京']=94648\n",
    "dict['重庆']=43223\n",
    "dict['浙江省']=68805\n",
    "dict['辽宁省']=61996\n",
    "dict['河南省']=34211\n",
    "dict['海南省']=35663\n",
    "dict['湖南省']=36943\n",
    "dict['甘肃省']=24539\n",
    "dict['陕西省']=43117\n",
    "dict['内蒙古自治区']=67836\n",
    "dict['江苏省']=75354\n",
    "dict['天津']=100105\n",
    "dict['上海']=90993\n",
    "dict['云南省']=25322\n",
    "dict['湖北省']=42826\n",
    "dict['江西省']=31930\n",
    "dict['吉林省']=47428\n",
    "dict['山西省']=34984\n",
    "dict['宁夏回族自治区']=39613\n",
    "dict['西藏自治区']=26326\n",
    "dict['新疆维吾尔自治区']=37553\n",
    "dict['广西壮族自治区']=30741\n",
    "dict['青海省']=36875\n",
    "ave=0\n",
    "total=0\n",
    "for key in dict:\n",
    "    total+=dict[key]\n",
    "ave = total/len(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''数据归一化'''\n",
    "def std_data(submit,train):\n",
    "    mean_list=submit.mean()\n",
    "    std_list=submit.std()\n",
    "    submit_std = submit\n",
    "    train_std = train\n",
    "    for col in submit.columns:\n",
    "        if col=='customer_id':\n",
    "            print('get it!')\n",
    "            pass\n",
    "        else:\n",
    "            submit_std[col] = (submit[col] - mean_list[col]) / std_list[col]\n",
    "            train_std[col] = (train[col] - mean_list[col]) / std_list[col]\n",
    "    return submit_std,train_std "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看结果的最大值，最小值，平均值\n",
    "def show(result):\n",
    "    print('max: ',str(max(result)))\n",
    "    print('min: ',str(min(result)))\n",
    "    print('mean: ',str((sum(result))/(len(result))))\n",
    "    print(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#官方的loss评估\n",
    "def logloss(y_true, y_pred,deta = 3.6, eps=1e-15):\n",
    "    # Prepare numpy array data\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    assert (len(y_true) and len(y_true) == len(y_pred))\n",
    "    # Clip y_pred between eps and 1-eps\n",
    "    p = np.clip(y_pred, eps, 1-eps)\n",
    "    loss = np.sum(- y_true * np.log(p) * deta - (1 - y_true) * np.log(1-p))\n",
    "    return loss / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "卡方特征选取\n",
    "\"\"\"\n",
    "def chi_square_test(X,y,select_k=10):\n",
    "    if select_k >= 1:\n",
    "        sel_ = SelectKBest(k=select_k).fit(X,y)\n",
    "        col = X.columns[sel_.get_support()]\n",
    "    elif 0 < select_k < 1:\n",
    "        sel_ = SelectPercentile(percentile=select_k*100).fit(X,y)\n",
    "        col = X.columns[sel_.get_support()]   \n",
    "    else:\n",
    "        raise ValueError(\"select_k must be a positive number\")  \n",
    "    \n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "把结果保存成指定格式的文件\n",
    "示例：save_result2csv(y_submit,submit,'./round1_diac2019_test.csv')\n",
    "其中y_submit为模型输出结果，submit为读入数据时生成的一个变量，第三个参数为保存的文件路劲\n",
    "'''\n",
    "def save_result2csv(ys_submit,submit,csv_name):\n",
    "    all_customers = pd.DataFrame(trian[['customer_id']]).drop_duplicates(['customer_id']).dropna()\n",
    "    submits_df = submit[['customer_id']]\n",
    "    submits_df['result'] = ys_submit\n",
    "    all_customers = pd.merge(all_customers,submits_df,on=['customer_id'],how='left',copy=False)\n",
    "    all_customers = all_customers.sort_values(['customer_id'])\n",
    "    all_customers['customer_id'] = all_customers['customer_id'].astype('int64')\n",
    "    all_customers['result'] = all_customers['result'].fillna(0)\n",
    "    all_customers.to_csv(csv_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(929807, 1)\n",
      "train date gap 2013-12-31 23:59:44 2013-01-01 00:00:18\n"
     ]
    }
   ],
   "source": [
    "# 读取原始数据\n",
    "trian = pd.read_csv('../Data/round1_diac2019_train.csv',low_memory=False)\n",
    "all_customer = pd.DataFrame(trian[['customer_id']]).drop_duplicates(['customer_id']).dropna()\n",
    "print(all_customer.shape)\n",
    "print('train date gap',trian.order_pay_time.max(),trian.order_pay_time.min())\n",
    "\n",
    "'''\n",
    "提供了2013年全年的数据，2013-12-31 23:59:44 2013-01-01 00:00:18\n",
    "'''\n",
    "trian['order_pay_time'] = pd.to_datetime(trian['order_pay_time'])\n",
    "trian['order_pay_date'] = trian['order_pay_time'].dt.date\n",
    "validata_date_begin = trian['order_pay_date'].max() - datetime.timedelta(days=180)\n",
    "\n",
    "trian[\"GDP\"]=ave\n",
    "for key in dict:\n",
    "    trian[\"GDP\"][trian[\"customer_province\"]==key]=dict[key]\n",
    "\n",
    "train_history = trian[(trian['order_pay_date'].astype(str)<='2013-07-03')]\n",
    "online_history = trian[(trian['order_pay_date'].astype(str)<='2013-12-31')]\n",
    "train_label = trian[trian['order_pay_date'].astype(str)>='2013-07-04']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 相关数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除类别唯一的特征\n",
    "for df in [train_history,online_history]:\n",
    "    df.drop(['order_detail_id','order_id','goods_id','member_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_total_discount 0.9973312428068654\n",
      "order_status 0.9455239937951395\n",
      "order_detail_status 0.9441710821625088\n",
      "order_detail_goods_num 0.9926961304874004\n",
      "order_total_discount 0.9873889141516714\n",
      "order_status 0.9128238881064168\n",
      "order_detail_status 0.9116224583836309\n",
      "order_detail_goods_num 0.9924087916386773\n"
     ]
    }
   ],
   "source": [
    "# 删除某一类别占比超过90%的列\n",
    "for df in [train_history,online_history]:\n",
    "    good_cols = list(df.columns)\n",
    "    for col in df.columns:\n",
    "        rate = df[col].value_counts(normalize=True, dropna=False).values[0]\n",
    "        if rate > 0.9:\n",
    "            good_cols.remove(col)\n",
    "            print(col,rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#删除异常值\n",
    "#trian = trian[trian[]]\n",
    "train_history = train_history[train_history['order_amount']<=2000]\n",
    "train_history = train_history[train_history['order_total_payment']<=2000]\n",
    "#online_history = online_history[online_history['order_amount']<=2000]\n",
    "#online_history = online_history[online_history['order_total_payment']<=2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#添加一些特征 \n",
    "train_history['goods_num'] = train_history['order_total_num']*train_history['order_count']\n",
    "train_label['goods_num'] = train_label['order_total_num']*train_label['order_count']\n",
    "online_history['goods_num'] = online_history['order_total_num']*online_history['order_count']\n",
    "\n",
    "train_history['dis_rate'] = (train_history['order_total_discount'])/train_history['order_amount']\n",
    "train_label['dis_rate'] = (train_label['order_total_discount'])/train_label['order_amount']\n",
    "online_history['dis_rate'] = (online_history['order_total_discount'])/online_history['order_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_history,train_label,online_history]:\n",
    "    df['log_order_amount'] = np.log(df['order_amount'].values+1)\n",
    "    df['log_goods_price'] = np.log(df['goods_price'].values+1)\n",
    "    df['pay_deleta_time'] = ( pd.to_datetime(df['goods_list_time'])  - pd.to_datetime(df['order_pay_time']))\n",
    "    df['goods_list_time'] = ( pd.to_datetime(df['goods_delist_time'])-pd.to_datetime(df['goods_list_time']))\n",
    "    df['pay_deleta_time'] = df['pay_deleta_time'].dt.days+1\n",
    "    df['goods_list_time'] = df['goods_list_time'].dt.days+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单的特征生成部分代码\n",
    "def make_feature_and_label(date1,date2,isSubmit):\n",
    "    date1['count'] = 1\n",
    "    # 统计这个用户出现了多少次\n",
    "    customer_id = date1.groupby(['customer_id'],as_index=False)['count'].agg({'count':'count'})\n",
    "    # 统计这个用户购买商品的价格信息\n",
    "    good_price = date1.groupby(['customer_id'],as_index=False)['goods_price'].agg({'goods_price_max':'max',\n",
    "                                                                                    'goods_price_min':'min',\n",
    "                                                                                    'goods_price_mean':'mean'})\n",
    "    \n",
    "    #添加用户性别\n",
    "    customer_gender= date1.groupby(['customer_id'],as_index=False)['customer_gender'].mean()\n",
    "    customer_gender = customer_gender.fillna(0)\n",
    "    \n",
    "    \n",
    "    #添加用户所属城市\n",
    "    class_le = LabelEncoder()\n",
    "    date1['customer_city'].fillna('未知',inplace=True)\n",
    "    date1['customer_city_l'] = class_le.fit_transform(date1['customer_city'])\n",
    "    customer_city_l = date1.groupby(['customer_id'],as_index=False)['customer_city_l'].max()\n",
    "    \n",
    "    #添加用户所属省份\n",
    "    date1['customer_province'].fillna('未知',inplace=True)\n",
    "    date1['customer_province_l'] = class_le.fit_transform(date1['customer_province'])\n",
    "    customer_province_l = date1.groupby(['customer_id'],as_index=False)['customer_province_l'].max()\n",
    "    \n",
    "    #统计该用户是不是会员\n",
    "    is_member = date1.groupby(['customer_id'],as_index=False)['is_member_actived'].median()\n",
    "    is_member = is_member.fillna(0)\n",
    "    \n",
    "    #统计用户购物车内商品数量\n",
    "    goods = date1.groupby(['customer_id'],as_index=False)['order_count'].max()\n",
    "    goods=goods.fillna(0)\n",
    "    \n",
    "    #统计用户花了多少钱及多少个\n",
    "    order_total_num = date1.groupby(['customer_id'],as_index=False)['order_total_num'].mean()\n",
    "    order_total_num = order_total_num.fillna(0)\n",
    "    \n",
    "    order_count = date1.groupby(['customer_id'],as_index=False)['order_count'].mean()\n",
    "    order_count = order_count.fillna(0)\n",
    "    \n",
    "    total_num = order_count*order_total_num\n",
    "    \n",
    "    order_amount = date1.groupby(['customer_id'],as_index=False)['order_amount'].max()\n",
    "    order_amount = order_amount.fillna(0)\n",
    "    \n",
    "    order_total_payment = date1.groupby(['customer_id'],as_index=False)['order_total_payment'].mean()\n",
    "    order_total_payment = order_total_payment.fillna(0)\n",
    "    order_total_payment_level = order_total_payment\n",
    "    order_total_payment_level['order_total_payment'] = order_total_payment['order_total_payment']>293\n",
    "      \n",
    "    \n",
    "    goods_num = date1.groupby(['customer_id'],as_index=False)['goods_num'].mean()\n",
    "    goods_num.fillna(0)\n",
    "     \n",
    "    gdp = date1.groupby(['customer_id'],as_index=False)['GDP'].mean()\n",
    "    \n",
    "    goods_list_time = date1.groupby(['customer_id'],as_index=False)['goods_list_time'].agg({'goods_list_time_last':'max',\n",
    "                                                                                           'goods_list_time_mean':'mean'})\n",
    "    \n",
    "        \n",
    "    pay_deleta_time = date1.groupby(['customer_id'],as_index=False)['pay_deleta_time'].agg({'pay_deleta_time_last':'max',\n",
    "                                                                                            'pay_deleta_time_first':'min',\n",
    "                                                                                           'pay_deleta_time_mean':'mean'})\n",
    "    \n",
    "    # 统计这个用户的订单最后一次购买时间\n",
    "    last_time = date1.groupby(['customer_id'],as_index=False)['order_pay_date'].agg({'order_pay_date_last':'max','order_pay_date_first':'min'})\n",
    "    # 当然这里面还可以构造更多的特征 \n",
    "    '''\n",
    "                    在这里疯狂加特征！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
    "    '''\n",
    "    data = pd.merge(customer_id,good_price,on=['customer_id'],how='left',copy=False)\n",
    "    data = pd.merge(data,is_member,on=['customer_id'],how='left',copy=False)\n",
    "    data = pd.merge(data,customer_gender,on=['customer_id'],how='left',copy=False)\n",
    "    data = pd.merge(data,gdp,on=['customer_id'],how='left',copy=False)\n",
    "    data = pd.merge(data,pay_deleta_time,on=['customer_id'],how='left',copy=False)\n",
    "    data = pd.merge(data,goods_list_time,on=['customer_id'],how='left',copy=False)\n",
    "    data = pd.merge(data,goods_num,on=['customer_id'],how='left',copy=False)\n",
    "    data = pd.merge(data,order_total_num,on=['customer_id'],how='left',copy=False)\n",
    "    data = pd.merge(data,order_total_payment_level,on=['customer_id'],how='left',copy=False)\n",
    "    data = pd.merge(data,customer_province_l,on=['customer_id'],how='left',copy=False)\n",
    "    data = pd.merge(data,last_time,on=['customer_id'],how='left',copy=False)\n",
    "    data['long_time'] = pd.to_datetime(data['order_pay_date_last']) - pd.to_datetime(data['order_pay_date_first'])\n",
    "    data['long_time'] = data['long_time'].dt.days + 1\n",
    "    del data['order_pay_date_first']\n",
    "    if isSubmit==False:\n",
    "        data['order_pay_date_last'] = pd.to_datetime(date2['order_pay_date'].min()) - pd.to_datetime(data['order_pay_date_last'])\n",
    "        data['order_pay_date_last'] = data['order_pay_date_last'].dt.days + 1\n",
    "        data['label'] = 0\n",
    "        data.loc[data['customer_id'].isin(list(date2['customer_id'].unique())),'label'] = 1\n",
    "        print(data['label'].mean())\n",
    "    else:\n",
    "        data['order_pay_date_last'] = pd.to_datetime('2013-12-31') - pd.to_datetime(data['order_pay_date_last'])\n",
    "        data['order_pay_date_last'] = data['order_pay_date_last'].dt.days + 1\n",
    "    print(data.shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1627399848929293\n",
      "(374659, 20)\n",
      "(929807, 19)\n"
     ]
    }
   ],
   "source": [
    "train = make_feature_and_label(train_history,train_label,False)\n",
    "submit = make_feature_and_label(online_history,None,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train.pop('label')\n",
    "feature = [x for x in train.columns if x not in ['customer_id']]\n",
    "X = train[feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第一次训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['count', 'goods_price_mean', 'goods_price_min', 'goods_price_max',\n",
      "       'is_member_actived', 'customer_gender', 'GDP', 'pay_deleta_time_last',\n",
      "       'pay_deleta_time_mean', 'pay_deleta_time_first', 'goods_list_time_last',\n",
      "       'goods_list_time_mean', 'goods_num', 'order_total_num',\n",
      "       'order_total_payment', 'customer_province_l', 'order_pay_date_last',\n",
      "       'long_time'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#第一次训练，利用卡方特征选取18个特征进行训练LGB和XGB\n",
    "x_columns = chi_square_test(X, y, 18)\n",
    "print(x_columns)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X[x_columns], y, test_size=0.25, random_state=42,stratify=y)\n",
    "submit_df = submit[['customer_id']]\n",
    "X_submit0 = submit[feature]\n",
    "X_submit = X_submit0[x_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#针对本次问题对loss function做了改动\n",
    "def customObj(real, predict):\n",
    "    deta = 3.6\n",
    "    grad = -1*( ((deta*real)/predict) ) - ( (real - 1)/( 1-predict ) )\n",
    "    hess = np.power(np.abs(grad), 1)\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [     1      2      3 ... 280990 280991 280992]\n",
      "[[22053  7473]\n",
      " [ 2216  3383]]\n",
      "Accuracy : 0.7242\n",
      "AUC Score (Train): 0.732725\n",
      "logloss SCore: 0.851799\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[22161  7148]\n",
      " [ 2326  3490]]\n",
      "Accuracy : 0.7303\n",
      "AUC Score (Train): 0.733098\n",
      "logloss SCore: 0.859362\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[22146  7258]\n",
      " [ 2294  3426]]\n",
      "Accuracy : 0.728\n",
      "AUC Score (Train): 0.734311\n",
      "logloss SCore: 0.853793\n",
      ">>> [     0      1      2 ... 280990 280991 280993]\n",
      "[[22070  7359]\n",
      " [ 2337  3358]]\n",
      "Accuracy : 0.7239\n",
      "AUC Score (Train): 0.726135\n",
      "logloss SCore: 0.860625\n",
      ">>> [     0      1      4 ... 280990 280992 280993]\n",
      "[[22048  7378]\n",
      " [ 2312  3386]]\n",
      "Accuracy : 0.7241\n",
      "AUC Score (Train): 0.727367\n",
      "logloss SCore: 0.860627\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[22124  7315]\n",
      " [ 2291  3394]]\n",
      "Accuracy : 0.7265\n",
      "AUC Score (Train): 0.731978\n",
      "logloss SCore: 0.854650\n",
      ">>> [     0      2      3 ... 280991 280992 280993]\n",
      "[[22054  7315]\n",
      " [ 2382  3373]]\n",
      "Accuracy : 0.7239\n",
      "AUC Score (Train): 0.727795\n",
      "logloss SCore: 0.861290\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[22063  7300]\n",
      " [ 2410  3351]]\n",
      "Accuracy : 0.7236\n",
      "AUC Score (Train): 0.722908\n",
      "logloss SCore: 0.865892\n"
     ]
    }
   ],
   "source": [
    "#8折 训练rf \n",
    "n_splits = 8\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=1234)\n",
    "y_pp_rf = np.zeros(len(X_submit))\n",
    "y_pp_rf_stacking = np.zeros(len(y_train))\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    print ( \">>>\", train_index )\n",
    "    rf = RandomForestClassifier(n_estimators=50, max_depth=8,random_state=10,class_weight='balanced_subsample')\n",
    "        \n",
    "    rf_model = rf.fit(X_train.iloc[train_index], y_train.iloc[train_index]) \n",
    "\n",
    "    #plot_importance(xgboost_model)\n",
    "    y_pred = rf_model.predict(X_train.iloc[test_index]) \n",
    "    y_predprob = rf_model.predict_proba(X_train.iloc[test_index])[:, 1] \n",
    "    \n",
    "    y_pp_rf_stacking[test_index] = y_predprob \n",
    "    \n",
    "    print(metrics.confusion_matrix(y_train.iloc[test_index].values, y_pred))\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_train.iloc[test_index].values, y_pred))  \n",
    "    auc = metrics.roc_auc_score(y_train.iloc[test_index], y_predprob)\n",
    "    print(\"AUC Score (Train): %f\" % auc) \n",
    "    loglossscore = logloss(y_train.iloc[test_index],y_predprob)\n",
    "    print(\"logloss SCore: %f\" %loglossscore)\n",
    "    \n",
    "    y_pp_rf += rf_model.predict_proba(X_submit)[:, 1] / n_splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [     1      2      3 ... 280990 280991 280992]\n",
      "[[24773  4753]\n",
      " [ 2854  2745]]\n",
      "Accuracy : 0.7834\n",
      "AUC Score (Train): 0.733810\n",
      "logloss SCore: 0.844603\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[24969  4340]\n",
      " [ 3031  2785]]\n",
      "Accuracy : 0.7901\n",
      "AUC Score (Train): 0.732960\n",
      "logloss SCore: 0.857281\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[24978  4426]\n",
      " [ 2928  2792]]\n",
      "Accuracy : 0.7906\n",
      "AUC Score (Train): 0.735260\n",
      "logloss SCore: 0.849133\n",
      ">>> [     0      1      2 ... 280990 280991 280993]\n",
      "[[24731  4698]\n",
      " [ 2928  2767]]\n",
      "Accuracy : 0.7829\n",
      "AUC Score (Train): 0.727755\n",
      "logloss SCore: 0.853516\n",
      ">>> [     0      1      4 ... 280990 280992 280993]\n",
      "[[24735  4691]\n",
      " [ 2946  2752]]\n",
      "Accuracy : 0.7826\n",
      "AUC Score (Train): 0.727615\n",
      "logloss SCore: 0.854285\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[24876  4563]\n",
      " [ 2929  2756]]\n",
      "Accuracy : 0.7867\n",
      "AUC Score (Train): 0.732903\n",
      "logloss SCore: 0.849702\n",
      ">>> [     0      2      3 ... 280991 280992 280993]\n",
      "[[24783  4586]\n",
      " [ 2981  2774]]\n",
      "Accuracy : 0.7846\n",
      "AUC Score (Train): 0.729743\n",
      "logloss SCore: 0.856111\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[24807  4556]\n",
      " [ 3052  2709]]\n",
      "Accuracy : 0.7834\n",
      "AUC Score (Train): 0.724538\n",
      "logloss SCore: 0.859879\n"
     ]
    }
   ],
   "source": [
    "#8折 训练xgb 这里没有在数据中增强正样本比例，而是跟换了loss function\n",
    "n_splits = 8\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=1234)\n",
    "y_pp_xgb = np.zeros(len(X_submit))\n",
    "y_pp_xgb_stacking = np.zeros(len(y_train))\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    print ( \">>>\", train_index )\n",
    "    xgboost = xgb.XGBClassifier(learning_rate =0.01,\n",
    "                                 n_estimators=250,\n",
    "                                 max_depth=8,\n",
    "                                 min_child_weight=1,\n",
    "                                 eta = 0.01,\n",
    "                                 gamma=0,\n",
    "                                 subsample=0.8,\n",
    "                                 colsample_bytree=0.8,\n",
    "                                 objective= customObj,\n",
    "                                 nthread=6,\n",
    "                                 scale_pos_weight=1.2,\n",
    "                                 seed=66)\n",
    "    xgboost_model = xgboost.fit(X_train.iloc[train_index], y_train.iloc[train_index]) \n",
    "\n",
    "    #plot_importance(xgboost_model)\n",
    "    y_pred = xgboost_model.predict(X_train.iloc[test_index]) \n",
    "    y_predprob = xgboost_model.predict_proba(X_train.iloc[test_index])[:, 1] \n",
    "    \n",
    "    y_pp_xgb_stacking[test_index] = y_predprob \n",
    "    \n",
    "    print(metrics.confusion_matrix(y_train.iloc[test_index].values, y_pred))\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_train.iloc[test_index].values, y_pred))  \n",
    "    auc = metrics.roc_auc_score(y_train.iloc[test_index], y_predprob)\n",
    "    print(\"AUC Score (Train): %f\" % auc) \n",
    "    loglossscore = logloss(y_train.iloc[test_index],y_predprob)\n",
    "    print(\"logloss SCore: %f\" %loglossscore)\n",
    "    \n",
    "    y_pp_xgb += xgboost_model.predict_proba(X_submit)[:, 1] / n_splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [     1      2      3 ... 280990 280991 280992]\n",
      "[[25843  3683]\n",
      " [ 3162  2437]]\n",
      "Accuracy : 0.8051\n",
      "AUC Score (Train): 0.733684\n",
      "logloss SCore: 0.833440\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[25900  3409]\n",
      " [ 3346  2470]]\n",
      "Accuracy : 0.8077\n",
      "AUC Score (Train): 0.734026\n",
      "logloss SCore: 0.849109\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[25836  3568]\n",
      " [ 3187  2533]]\n",
      "Accuracy : 0.8077\n",
      "AUC Score (Train): 0.735134\n",
      "logloss SCore: 0.839221\n",
      ">>> [     0      1      2 ... 280990 280991 280993]\n",
      "[[25768  3661]\n",
      " [ 3246  2449]]\n",
      "Accuracy : 0.8034\n",
      "AUC Score (Train): 0.728489\n",
      "logloss SCore: 0.844683\n",
      ">>> [     0      1      4 ... 280990 280992 280993]\n",
      "[[25781  3645]\n",
      " [ 3268  2430]]\n",
      "Accuracy : 0.8032\n",
      "AUC Score (Train): 0.728263\n",
      "logloss SCore: 0.845450\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[25914  3525]\n",
      " [ 3250  2435]]\n",
      "Accuracy : 0.8071\n",
      "AUC Score (Train): 0.733058\n",
      "logloss SCore: 0.840016\n",
      ">>> [     0      2      3 ... 280991 280992 280993]\n",
      "[[25851  3518]\n",
      " [ 3266  2489]]\n",
      "Accuracy : 0.8069\n",
      "AUC Score (Train): 0.730759\n",
      "logloss SCore: 0.847790\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[25742  3621]\n",
      " [ 3337  2424]]\n",
      "Accuracy : 0.8019\n",
      "AUC Score (Train): 0.724967\n",
      "logloss SCore: 0.853353\n"
     ]
    }
   ],
   "source": [
    "#8折 训练lgb 这里在数据层面增强了正样本比例，但没有更换loss function\n",
    "n_splits = 8\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=1234)\n",
    "y_pp_lgb = np.zeros(len(X_submit))\n",
    "y_pp_lgb_stacking = np.zeros(len(y_train))\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    print ( \">>>\", train_index )\n",
    "    lightgbm = lgb.LGBMClassifier(learning_rate =0.01,\n",
    "                                 n_estimators=250,\n",
    "                                 max_depth=12,\n",
    "                                 min_child_weight=1,\n",
    "                                 eta = 0.01,\n",
    "                                 alpha = 0.01,\n",
    "                                 gamma=0.0,\n",
    "                                 subsample=0.7,\n",
    "                                 colsample_bytree=0.7,\n",
    "                                 nthread=6,\n",
    "                                 scale_pos_weight=3.5,\n",
    "                                 seed=66)\n",
    "    lightgbm_model = lightgbm.fit(X_train.iloc[train_index], y_train.iloc[train_index]) \n",
    "\n",
    "    y_pred = lightgbm_model.predict(X_train.iloc[test_index]) \n",
    "    y_predprob = lightgbm_model.predict_proba(X_train.iloc[test_index])[:, 1] \n",
    "    \n",
    "    y_pp_lgb_stacking[test_index] = y_predprob\n",
    "    \n",
    "    print(metrics.confusion_matrix(y_train.iloc[test_index].values, y_pred))\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_train.iloc[test_index].values, y_pred))  \n",
    "    auc = metrics.roc_auc_score(y_train.iloc[test_index], y_predprob)\n",
    "    print(\"AUC Score (Train): %f\" % auc) \n",
    "    loglossscore = logloss(y_train.iloc[test_index],y_predprob)\n",
    "    print(\"logloss SCore: %f\" %loglossscore)\n",
    "    \n",
    "    y_pp_lgb += lightgbm_model.predict_proba(X_submit)[:, 1] / n_splits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二次训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['count', 'is_member_actived', 'customer_gender',\n",
      "       'pay_deleta_time_first', 'goods_list_time_last', 'goods_num',\n",
      "       'order_total_payment', 'order_pay_date_last', 'long_time'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#第二次训练，利用卡方特征选取8个特征进行训练LGB,XGB,RF\n",
    "x_columns0 = chi_square_test(X, y, 9)\n",
    "print(x_columns0)\n",
    "X_train0, X_valid0, y_train0, y_valid0 = train_test_split(X[x_columns0], y, test_size=0.25, random_state=42,stratify=y)\n",
    "submit_df = submit[['customer_id']]\n",
    "X_submit00 = submit[feature]\n",
    "X_submit01 = X_submit00[x_columns0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [     1      2      3 ... 280990 280991 280992]\n",
      "[[23013  6513]\n",
      " [ 2463  3136]]\n",
      "Accuracy : 0.7445\n",
      "AUC Score (Train): 0.725132\n",
      "logloss SCore: 0.852528\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[23082  6227]\n",
      " [ 2599  3217]]\n",
      "Accuracy : 0.7487\n",
      "AUC Score (Train): 0.722514\n",
      "logloss SCore: 0.863438\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[22937  6467]\n",
      " [ 2490  3230]]\n",
      "Accuracy : 0.745\n",
      "AUC Score (Train): 0.726793\n",
      "logloss SCore: 0.853944\n",
      ">>> [     0      1      2 ... 280990 280991 280993]\n",
      "[[22963  6466]\n",
      " [ 2558  3137]]\n",
      "Accuracy : 0.7431\n",
      "AUC Score (Train): 0.719935\n",
      "logloss SCore: 0.860839\n",
      ">>> [     0      1      4 ... 280990 280992 280993]\n",
      "[[22943  6483]\n",
      " [ 2551  3147]]\n",
      "Accuracy : 0.7428\n",
      "AUC Score (Train): 0.719467\n",
      "logloss SCore: 0.862822\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[22930  6509]\n",
      " [ 2534  3151]]\n",
      "Accuracy : 0.7425\n",
      "AUC Score (Train): 0.722479\n",
      "logloss SCore: 0.857567\n",
      ">>> [     0      2      3 ... 280991 280992 280993]\n",
      "[[22782  6587]\n",
      " [ 2568  3187]]\n",
      "Accuracy : 0.7394\n",
      "AUC Score (Train): 0.717166\n",
      "logloss SCore: 0.864787\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[22903  6460]\n",
      " [ 2609  3152]]\n",
      "Accuracy : 0.7418\n",
      "AUC Score (Train): 0.717158\n",
      "logloss SCore: 0.865642\n"
     ]
    }
   ],
   "source": [
    "#8折 训练RF\n",
    "n_splits = 8\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=1234)\n",
    "y_pp_rf0 = np.zeros(len(X_submit))\n",
    "y_pp_rf_stacking0 = np.zeros(len(y_train0))\n",
    "for train_index, test_index in kf.split(X_train0):\n",
    "    print ( \">>>\", train_index )\n",
    "    rf0 = RandomForestClassifier(n_estimators=20, max_depth=12,random_state=10,class_weight='balanced')\n",
    "       \n",
    "    rf_model0 = rf0.fit(X_train0.iloc[train_index], y_train0.iloc[train_index]) \n",
    "\n",
    "    #plot_importance(xgboost_model)\n",
    "    y_pred = rf_model0.predict(X_train0.iloc[test_index]) \n",
    "    y_predprob = rf_model0.predict_proba(X_train0.iloc[test_index])[:, 1] \n",
    "    \n",
    "    y_pp_rf_stacking0[test_index] = y_predprob \n",
    "    \n",
    "    print(metrics.confusion_matrix(y_train0.iloc[test_index].values, y_pred))\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_train0.iloc[test_index].values, y_pred))  \n",
    "    auc = metrics.roc_auc_score(y_train0.iloc[test_index], y_predprob)\n",
    "    print(\"AUC Score (Train): %f\" % auc) \n",
    "    loglossscore = logloss(y_train0.iloc[test_index],y_predprob)\n",
    "    print(\"logloss SCore: %f\" %loglossscore)\n",
    "    \n",
    "    y_pp_rf0 += rf_model0.predict_proba(X_submit01)[:, 1] / n_splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [     1      2      3 ... 280990 280991 280992]\n",
      "[[25104  4422]\n",
      " [ 2950  2649]]\n",
      "Accuracy : 0.7901\n",
      "AUC Score (Train): 0.730351\n",
      "logloss SCore: 0.842376\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[25209  4100]\n",
      " [ 3133  2683]]\n",
      "Accuracy : 0.7941\n",
      "AUC Score (Train): 0.729703\n",
      "logloss SCore: 0.855509\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[25139  4265]\n",
      " [ 2993  2727]]\n",
      "Accuracy : 0.7934\n",
      "AUC Score (Train): 0.731339\n",
      "logloss SCore: 0.846556\n",
      ">>> [     0      1      2 ... 280990 280991 280993]\n",
      "[[25109  4320]\n",
      " [ 3059  2636]]\n",
      "Accuracy : 0.7899\n",
      "AUC Score (Train): 0.724204\n",
      "logloss SCore: 0.851460\n",
      ">>> [     0      1      4 ... 280990 280992 280993]\n",
      "[[25156  4270]\n",
      " [ 3064  2634]]\n",
      "Accuracy : 0.7912\n",
      "AUC Score (Train): 0.724866\n",
      "logloss SCore: 0.852156\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[25210  4229]\n",
      " [ 3045  2640]]\n",
      "Accuracy : 0.7929\n",
      "AUC Score (Train): 0.728443\n",
      "logloss SCore: 0.847872\n",
      ">>> [     0      2      3 ... 280991 280992 280993]\n",
      "[[25154  4215]\n",
      " [ 3082  2673]]\n",
      "Accuracy : 0.7923\n",
      "AUC Score (Train): 0.723872\n",
      "logloss SCore: 0.854759\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[25109  4254]\n",
      " [ 3145  2616]]\n",
      "Accuracy : 0.7893\n",
      "AUC Score (Train): 0.721816\n",
      "logloss SCore: 0.857897\n"
     ]
    }
   ],
   "source": [
    "#8折 训练xgb 这里没有在数据中增强正样本比例，而是跟换了loss function\n",
    "n_splits = 8\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=1234)\n",
    "y_pp_xgb0 = np.zeros(len(X_submit01))\n",
    "y_pp_xgb_stacking0 = np.zeros(len(y_train0))\n",
    "for train_index, test_index in kf.split(X_train0):\n",
    "    print ( \">>>\", train_index )\n",
    "    xgboost0 = xgb.XGBClassifier(learning_rate =0.01,\n",
    "                                 n_estimators=300,\n",
    "                                 max_depth=8,\n",
    "                                 min_child_weight=1,\n",
    "                                 eta = 0.01,\n",
    "                                 gamma=0.01,\n",
    "                                 subsample=0.8,\n",
    "                                 colsample_bytree=0.8,\n",
    "                                 objective= customObj,\n",
    "                                 nthread=6,\n",
    "                                 scale_pos_weight=1,\n",
    "                                 seed=66)\n",
    "    xgboost_model0 = xgboost0.fit(X_train0.iloc[train_index], y_train0.iloc[train_index]) \n",
    "\n",
    "    #plot_importance(xgboost_model)\n",
    "    y_pred = xgboost_model0.predict(X_train0.iloc[test_index]) \n",
    "    y_predprob = xgboost_model0.predict_proba(X_train0.iloc[test_index])[:, 1] \n",
    "    \n",
    "    y_pp_xgb_stacking0[test_index]=y_predprob\n",
    "    \n",
    "    print(metrics.confusion_matrix(y_train0.iloc[test_index].values, y_pred))\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_train0.iloc[test_index].values, y_pred))  \n",
    "    auc = metrics.roc_auc_score(y_train0.iloc[test_index], y_predprob)\n",
    "    print(\"AUC Score (Train): %f\" % auc) \n",
    "    loglossscore = logloss(y_train0.iloc[test_index],y_predprob)\n",
    "    print(\"logloss SCore: %f\" %loglossscore)\n",
    "    \n",
    "    y_pp_xgb0 += xgboost_model0.predict_proba(X_submit01)[:, 1] / n_splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> [     1      2      3 ... 280990 280991 280992]\n",
      "[[27473  2053]\n",
      " [ 3794  1805]]\n",
      "Accuracy : 0.8335\n",
      "AUC Score (Train): 0.730949\n",
      "logloss SCore: 0.874133\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[27351  1958]\n",
      " [ 3979  1837]]\n",
      "Accuracy : 0.831\n",
      "AUC Score (Train): 0.730466\n",
      "logloss SCore: 0.899833\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[27424  1980]\n",
      " [ 3861  1859]]\n",
      "Accuracy : 0.8337\n",
      "AUC Score (Train): 0.731560\n",
      "logloss SCore: 0.883443\n",
      ">>> [     0      1      2 ... 280990 280991 280993]\n",
      "[[27277  2152]\n",
      " [ 3835  1860]]\n",
      "Accuracy : 0.8295\n",
      "AUC Score (Train): 0.725066\n",
      "logloss SCore: 0.890197\n",
      ">>> [     0      1      4 ... 280990 280992 280993]\n",
      "[[27295  2131]\n",
      " [ 3877  1821]]\n",
      "Accuracy : 0.8289\n",
      "AUC Score (Train): 0.724952\n",
      "logloss SCore: 0.891657\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[27432  2007]\n",
      " [ 3867  1818]]\n",
      "Accuracy : 0.8328\n",
      "AUC Score (Train): 0.729062\n",
      "logloss SCore: 0.885214\n",
      ">>> [     0      2      3 ... 280991 280992 280993]\n",
      "[[27412  1957]\n",
      " [ 3955  1800]]\n",
      "Accuracy : 0.8317\n",
      "AUC Score (Train): 0.725126\n",
      "logloss SCore: 0.897303\n",
      ">>> [     0      1      2 ... 280991 280992 280993]\n",
      "[[27332  2031]\n",
      " [ 3922  1839]]\n",
      "Accuracy : 0.8305\n",
      "AUC Score (Train): 0.722314\n",
      "logloss SCore: 0.902057\n"
     ]
    }
   ],
   "source": [
    "#8折 训练lgb 这里在数据层面增强了正样本比例，但没有更换loss function\n",
    "n_splits = 8\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=1234)\n",
    "y_pp_lgb0 = np.zeros(len(X_submit01))\n",
    "y_pp_lgb_stacking0 = np.zeros(len(y_train0))\n",
    "for train_index, test_index in kf.split(X_train0):\n",
    "    print ( \">>>\", train_index )\n",
    "    lightgbm0 = lgb.LGBMClassifier(learning_rate =0.03,\n",
    "                                 n_estimators=250,\n",
    "                                 max_depth=9,\n",
    "                                 min_child_weight=1,\n",
    "                                 eta = 0.03,\n",
    "                                 alpha = 0.1,\n",
    "                                 gamma=0.1,\n",
    "                                 subsample=0.8,\n",
    "                                 colsample_bytree=0.8,\n",
    "                                 nthread=6,\n",
    "                                 scale_pos_weight=2,\n",
    "                                 seed=66)\n",
    "    lightgbm_model0 = lightgbm0.fit(X_train0.iloc[train_index], y_train0.iloc[train_index]) \n",
    "\n",
    "    y_pred = lightgbm_model0.predict(X_train0.iloc[test_index]) \n",
    "    y_predprob = lightgbm_model0.predict_proba(X_train0.iloc[test_index])[:, 1] \n",
    "    \n",
    "    y_pp_lgb_stacking0[test_index] = y_predprob\n",
    "    \n",
    "    print(metrics.confusion_matrix(y_train0.iloc[test_index].values, y_pred))\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(y_train0.iloc[test_index].values, y_pred))  \n",
    "    auc = metrics.roc_auc_score(y_train0.iloc[test_index], y_predprob)\n",
    "    print(\"AUC Score (Train): %f\" % auc) \n",
    "    loglossscore = logloss(y_train0.iloc[test_index],y_predprob)\n",
    "    print(\"logloss SCore: %f\" %loglossscore)\n",
    "    \n",
    "    y_pp_lgb0 += lightgbm_model0.predict_proba(X_submit01)[:, 1] / n_splits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking 6次训练的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "[[42543  4285]\n",
      " [ 5952  3419]]\n",
      "Accuracy : 0.8178\n",
      "AUC Score (Train): 0.731909\n",
      "logloss SCore: 0.869212\n",
      "escape time: 3.563441514968872\n",
      "fold 1\n",
      "[[42576  4612]\n",
      " [ 5538  3473]]\n",
      "Accuracy : 0.8194\n",
      "AUC Score (Train): 0.729905\n",
      "logloss SCore: 0.849950\n",
      "escape time: 3.6103453636169434\n",
      "fold 2\n",
      "[[42971  4159]\n",
      " [ 5729  3340]]\n",
      "Accuracy : 0.8241\n",
      "AUC Score (Train): 0.735492\n",
      "logloss SCore: 0.848767\n",
      "escape time: 3.665201425552368\n",
      "fold 3\n",
      "[[42686  4415]\n",
      " [ 5640  3458]]\n",
      "Accuracy : 0.8211\n",
      "AUC Score (Train): 0.728812\n",
      "logloss SCore: 0.854995\n",
      "escape time: 3.6482439041137695\n",
      "fold 4\n",
      "[[42562  4456]\n",
      " [ 5678  3502]]\n",
      "Accuracy : 0.8197\n",
      "AUC Score (Train): 0.730155\n",
      "logloss SCore: 0.857884\n",
      "escape time: 3.57444167137146\n",
      "fold 5\n",
      "[[42744  4203]\n",
      " [ 5761  3491]]\n",
      "Accuracy : 0.8227\n",
      "AUC Score (Train): 0.736663\n",
      "logloss SCore: 0.857124\n",
      "escape time: 3.692126750946045\n",
      "fold 6\n",
      "[[42804  4309]\n",
      " [ 5713  3373]]\n",
      "Accuracy : 0.8217\n",
      "AUC Score (Train): 0.728857\n",
      "logloss SCore: 0.854840\n",
      "escape time: 3.575438976287842\n",
      "fold 7\n",
      "[[42755  4215]\n",
      " [ 5763  3466]]\n",
      "Accuracy : 0.8225\n",
      "AUC Score (Train): 0.733029\n",
      "logloss SCore: 0.857648\n",
      "escape time: 3.6482441425323486\n",
      "fold 8\n",
      "[[43007  4179]\n",
      " [ 5775  3238]]\n",
      "Accuracy : 0.8229\n",
      "AUC Score (Train): 0.728493\n",
      "logloss SCore: 0.852937\n",
      "escape time: 3.5854129791259766\n",
      "fold 9\n",
      "[[42449  4600]\n",
      " [ 5643  3506]]\n",
      "Accuracy : 0.8177\n",
      "AUC Score (Train): 0.729447\n",
      "logloss SCore: 0.857471\n",
      "escape time: 3.564467430114746\n",
      "0.8560041405182636\n"
     ]
    }
   ],
   "source": [
    "# 将两次训练的lgb,xgb,rf的结果进行stacking\n",
    "train_stack = np.vstack([y_pp_lgb_stacking,y_pp_lgb_stacking0,y_pp_xgb_stacking,y_pp_xgb_stacking0,y_pp_rf_stacking,y_pp_rf_stacking0]).transpose()\n",
    "test_stack = np.vstack([y_pp_lgb,y_pp_lgb0,y_pp_xgb,y_pp_xgb0,y_pp_rf,y_pp_rf0]).transpose()\n",
    "\n",
    "#train_stack = np.vstack([y_pp_lgb_stacking,y_pp_lgb_stacking0,y_pp_xgb_stacking,y_pp_xgb_stacking0]).transpose()\n",
    "#test_stack = np.vstack([y_pp_lgb,y_pp_lgb0,y_pp_xgb,y_pp_xgb0]).transpose()\n",
    "\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=666)\n",
    "oof_stack = np.zeros(train_stack.shape[0])\n",
    "predictions = np.zeros(test_stack.shape[0])\n",
    "predictions_final = np.zeros(X_submit.shape[0])\n",
    "y_predall = np.zeros(train_stack.shape[0])\n",
    "y_predproball = np.zeros(train_stack.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack,y_train)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    t1 = time.time()\n",
    "    trn_data, trn_y = train_stack[trn_idx], y_train.iloc[trn_idx]\n",
    "    val_data, val_y = train_stack[val_idx], y_train.iloc[val_idx]\n",
    "\n",
    "\n",
    "    clf_5 =  lgb.LGBMClassifier(learning_rate =0.01,\n",
    "                                 n_estimators=200,\n",
    "                                 max_depth=15,\n",
    "                                 min_child_weight=1,\n",
    "                                 eta = 0.01,\n",
    "                                 alpha = 0.01,\n",
    "                                 gamma=0.0,\n",
    "                                 subsample=0.8,\n",
    "                                 colsample_bytree=0.8,\n",
    "                                 nthread=6,\n",
    "                                 scale_pos_weight=3,\n",
    "                                 seed=66)\n",
    "                             \n",
    "    #clf_5 = LinearDiscriminantAnalysis()\n",
    "    #clf_5 = LogisticRegression()\n",
    "    \n",
    "    clf_5.fit(trn_data, trn_y)\n",
    "    \n",
    "    \n",
    "    y_pred = clf_5.predict(val_data) \n",
    "    y_predprob = clf_5.predict_proba(val_data)[:, 1] \n",
    "    y_predall[val_idx] = y_pred\n",
    "    y_predproball[val_idx] = y_predprob\n",
    "      \n",
    "    print(metrics.confusion_matrix(val_y, y_pred))\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(val_y, y_pred))  \n",
    "    auc = metrics.roc_auc_score(val_y, y_predprob)\n",
    "    print(\"AUC Score (Train): %f\" % auc) \n",
    "    loglossscore = logloss(val_y,y_predprob)\n",
    "    print(\"logloss SCore: %f\" %loglossscore)\n",
    "    \n",
    "    \n",
    "    oof_stack[val_idx] = np.array(clf_5.predict_proba(val_data)[:,1])\n",
    "    predictions += np.array(clf_5.predict_proba(test_stack)[:,1])/ 10\n",
    "    t2 = time.time()\n",
    "    print('escape time:',str(t2-t1))\n",
    "print(logloss(y_train, oof_stack))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_result2csv(predictions,submit,'./round1_diac2019_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据后处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把一年内购买次数超过10次的人，购买概率增加0.1\n",
    "res = pd.read_csv('./round1_diac2019_test.csv')\n",
    "id0=submit[\"customer_id\"][submit[\"count\"]>10]\n",
    "id1=np.array(id0)\n",
    "for i in trange(len(id1)):\n",
    "    res[\"result\"][res[\"customer_id\"]==id1[i]]+=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('./res_final.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
