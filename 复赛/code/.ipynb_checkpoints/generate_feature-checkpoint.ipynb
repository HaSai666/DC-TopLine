{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection  import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import datetime\n",
    "from tqdm import trange\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import warnings\n",
    "import time\n",
    "from numpy import *\n",
    "from sklearn.linear_model import  *\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "from xgboost import *\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.linear_model import  *\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sympy import *\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict={}\n",
    "dict['四川省']=32617\n",
    "dict['安徽省']=32001\n",
    "dict['福建省']=58145\n",
    "dict['河北省']=38909\n",
    "dict['贵州省']=23151\n",
    "dict['山东省']=56885\n",
    "dict['黑龙江省']=37697\n",
    "dict['广东省']=58833\n",
    "dict['北京']=94648\n",
    "dict['重庆']=43223\n",
    "dict['浙江省']=68805\n",
    "dict['辽宁省']=61996\n",
    "dict['河南省']=34211\n",
    "dict['海南省']=35663\n",
    "dict['湖南省']=36943\n",
    "dict['甘肃省']=24539\n",
    "dict['陕西省']=43117\n",
    "dict['内蒙古自治区']=67836\n",
    "dict['江苏省']=75354\n",
    "dict['天津']=100105\n",
    "dict['上海']=90993\n",
    "dict['云南省']=25322\n",
    "dict['湖北省']=42826\n",
    "dict['江西省']=31930\n",
    "dict['吉林省']=47428\n",
    "dict['山西省']=34984\n",
    "dict['宁夏回族自治区']=39613\n",
    "dict['西藏自治区']=26326\n",
    "dict['新疆维吾尔自治区']=37553\n",
    "dict['广西壮族自治区']=30741\n",
    "dict['青海省']=36875\n",
    "ave=0\n",
    "total=0\n",
    "for key in dict:\n",
    "    total+=dict[key]\n",
    "ave = total/len(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_month(s):\n",
    "    return s.apply(lambda x: int(x.split('-')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../Data/round12_diac2019_train.csv' does not exist: b'../Data/round12_diac2019_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-b2642a596fa2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 合并数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtemp1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../Data/round12_diac2019_train.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtemp2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../Data/round11_diac2019_train.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../Data/round112_diac2019_train.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../Data/round12_diac2019_train.csv' does not exist: b'../Data/round12_diac2019_train.csv'"
     ]
    }
   ],
   "source": [
    "# 合并数据\n",
    "temp1 = pd.read_csv('../Data/round12_diac2019_train.csv',low_memory=False)\n",
    "temp2 = pd.read_csv('../Data/round11_diac2019_train.csv',low_memory=False)\n",
    "temp = temp1.append(temp2)\n",
    "temp.to_csv('../Data/round112_diac2019_train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1826575, 1)\n",
      "train date gap 2013-12-31 23:59:44 2012-11-01 00:00:07\n"
     ]
    }
   ],
   "source": [
    "# 读取原始数据\n",
    "trian = pd.read_csv('../Data/round12_diac2019_train.csv',low_memory=False)\n",
    "all_customer = pd.DataFrame(trian[['customer_id']]).drop_duplicates(['customer_id']).dropna()\n",
    "print(all_customer.shape)\n",
    "print('train date gap',trian.order_pay_time.max(),trian.order_pay_time.min())\n",
    "\n",
    "trian['order_pay_time'] = pd.to_datetime(trian['order_pay_time'])\n",
    "trian['order_pay_date'] = trian['order_pay_time'].dt.date\n",
    "validata_date_begin = trian['order_pay_date'].max() - datetime.timedelta(days=180)\n",
    "\n",
    "trian['order_month'] = extract_month(trian['order_pay_date'].astype(str))\n",
    "\n",
    "trian[\"GDP\"]=ave\n",
    "for key in dict:\n",
    "    trian[\"GDP\"][trian[\"customer_province\"]==key]=dict[key]\n",
    "\n",
    "count_fea=['order_month','goods_id']\n",
    "for feat in count_fea:\n",
    "    col_name = '{}_count'.format(feat)\n",
    "    trian[col_name] = trian[feat].map(trian[feat].value_counts().astype(int))\n",
    "    trian.loc[trian[col_name] < 2, feat] = -1\n",
    "    trian[feat] += 1\n",
    "    trian[col_name] = trian[feat].map(trian[feat].value_counts().astype(int))\n",
    "    trian[col_name] = (trian[col_name] - trian[col_name].min()) / (trian[col_name].max() - trian[col_name].min())\n",
    "\n",
    "train_history = trian[(trian['order_pay_date'].astype(str)<='2013-10-31')]\n",
    "online_history = trian[(trian['order_pay_date'].astype(str)<='2013-12-31')]\n",
    "train_label = trian[trian['order_pay_date'].astype(str)>='2013-11-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 相关数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除类别唯一的特征\n",
    "for df in [train_history,online_history]:\n",
    "    df.drop(['order_detail_id','order_id','goods_id','member_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_customer_rate 0.9493003981690669\n",
      "order_detail_goods_num 0.9538791201830489\n",
      "order_detail_status 0.9102991657674617\n",
      "order_status 0.9126025565943487\n",
      "is_customer_rate 0.9408900062182921\n",
      "order_detail_goods_num 0.9584204020672824\n",
      "order_detail_status 0.9049027110138655\n",
      "order_status 0.9070163430226283\n"
     ]
    }
   ],
   "source": [
    "# 删除某一类别占比超过90%的列\n",
    "for df in [train_history,online_history]:\n",
    "    good_cols = list(df.columns)\n",
    "    for col in df.columns:\n",
    "        rate = df[col].value_counts(normalize=True, dropna=False).values[0]\n",
    "        if rate > 0.9:\n",
    "            good_cols.remove(col)\n",
    "            print(col,rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#删除异常值\n",
    "#trian = trian[trian[]]\n",
    "train_history = train_history[train_history['order_amount']<=2000]\n",
    "train_history = train_history[train_history['order_total_payment']<=2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#添加一些特征 \n",
    "train_history['goods_num'] = train_history['order_total_num']*train_history['order_count']\n",
    "train_label['goods_num'] = train_label['order_total_num']*train_label['order_count']\n",
    "online_history['goods_num'] = online_history['order_total_num']*online_history['order_count']\n",
    "\n",
    "train_history['dis_rate'] = (train_history['order_total_discount'])/train_history['order_amount']\n",
    "train_label['dis_rate'] = (train_label['order_total_discount'])/train_label['order_amount']\n",
    "online_history['dis_rate'] = (online_history['order_total_discount'])/online_history['order_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_history,train_label,online_history]:\n",
    "    df['log_order_amount'] = np.log(df['order_amount'].values+1)\n",
    "    df['log_goods_price'] = np.log(df['goods_price'].values+1)\n",
    "    df['pay_deleta_time'] = ( pd.to_datetime(df['goods_list_time'])  - pd.to_datetime(df['order_pay_time']))\n",
    "    df['goods_list_time'] = ( pd.to_datetime(df['goods_delist_time'])-pd.to_datetime(df['goods_list_time']))\n",
    "    df['pay_deleta_time'] = df['pay_deleta_time'].dt.days+1\n",
    "    df['goods_list_time'] = df['goods_list_time'].dt.days+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征工程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简单的特征生成部分代码\n",
    "def make_feature_and_label(date1,date2,isSubmit):\n",
    "    date1['count'] = 1\n",
    "    # 统计这个用户出现了多少次\n",
    "    customer_id = date1.groupby(['customer_id'],as_index=False)['count'].agg({'count':'count'})\n",
    "    # 统计这个用户购买商品的价格信息\n",
    "    good_price = date1.groupby(['customer_id'],as_index=False)['goods_price'].agg({'goods_price_max':'max',\n",
    "                                                                                    'goods_price_min':'min',\n",
    "                                                                                    'goods_price_mean':'mean',\n",
    "                                                                                    'goods_price_sum':'sum'})\n",
    "    \n",
    "    #添加用户性别\n",
    "    customer_gender= date1.groupby(['customer_id'],as_index=False)['customer_gender'].mean()\n",
    "    customer_gender = customer_gender.fillna(0)\n",
    "    \n",
    "    \n",
    "    class_le = LabelEncoder()    \n",
    "    #添加用户所属省份\n",
    "    date1['customer_province'].fillna('未知',inplace=True)\n",
    "    date1['customer_province_l'] = class_le.fit_transform(date1['customer_province'])\n",
    "    customer_province_l = date1.groupby(['customer_id'],as_index=False)['customer_province_l'].max()\n",
    "    \n",
    "    #gdp\n",
    "    gdp = date1.groupby(['customer_id'],as_index=False)['GDP'].max()\n",
    "    #月份特征\n",
    "    order_month_count = date1.groupby(['customer_id'],as_index=False)['order_month_count'].max()\n",
    "    \n",
    "    #商品特征\n",
    "    goods_id_count = date1.groupby(['customer_id'],as_index=False)['goods_id_count'].max()\n",
    "    \n",
    "    #统计该用户是不是会员\n",
    "    is_member = date1.groupby(['customer_id'],as_index=False)['is_member_actived'].median()\n",
    "    is_member = is_member.fillna(0)\n",
    "    \n",
    "    #统计用户购物车内商品数量\n",
    "    goods = date1.groupby(['customer_id'],as_index=False)['order_count'].max()\n",
    "    goods=goods.fillna(0)\n",
    "    \n",
    "    #统计用户花了多少钱及多少个\n",
    "    order_total_num = date1.groupby(['customer_id'],as_index=False)['order_total_num'].mean()\n",
    "    order_total_num = order_total_num.fillna(0)\n",
    "    \n",
    "    order_count = date1.groupby(['customer_id'],as_index=False)['order_count'].mean()\n",
    "    order_count = order_count.fillna(0)\n",
    "    \n",
    "    total_num = order_count*order_total_num\n",
    "    \n",
    "    order_amount = date1.groupby(['customer_id'],as_index=False)['order_amount'].max()\n",
    "    order_amount = order_amount.fillna(0)\n",
    "    \n",
    "    order_total_payment = date1.groupby(['customer_id'],as_index=False)['order_total_payment'].mean()\n",
    "    order_total_payment = order_total_payment.fillna(0)\n",
    "    order_total_payment_level = order_total_payment\n",
    "    order_total_payment_level['order_total_payment'] = order_total_payment['order_total_payment']>293\n",
    "      \n",
    "    \n",
    "    goods_num = date1.groupby(['customer_id'],as_index=False)['goods_num'].mean()\n",
    "    goods_num.fillna(0)\n",
    "    \n",
    "    goods_list_time = date1.groupby(['customer_id'],as_index=False)['goods_list_time'].agg({'goods_list_time_last':'max',\n",
    "                                                                                           'goods_list_time_mean':'mean'})\n",
    "    \n",
    "        \n",
    "    pay_deleta_time = date1.groupby(['customer_id'],as_index=False)['pay_deleta_time'].agg({'pay_deleta_time_last':'max',\n",
    "                                                                                            'pay_deleta_time_first':'min',\n",
    "                                                                                           'pay_deleta_time_mean':'mean'})\n",
    "    \n",
    "    # 统计这个用户的订单最后一次购买时间\n",
    "    last_time = date1.groupby(['customer_id'],as_index=False)['order_pay_date'].agg({'order_pay_date_last':'max','order_pay_date_first':'min'})\n",
    "    # 当然这里面还可以构造更多的特征 \n",
    "    '''\n",
    "                    在这里疯狂加特征！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
    "    '''\n",
    "    data = pd.merge(customer_id,good_price,on=['customer_id'],how='left',copy=False)\n",
    "    data = pd.merge(data,is_member,on=['customer_id'],how='left',copy=False)\n",
    "    data = pd.merge(data,customer_gender,on=['customer_id'],how='left',copy=False)\n",
    "    data = pd.merge(data,order_month_count,on=['customer_id'],how='left',copy=False)\n",
    "    data = pd.merge(data,goods_id_count,on=['customer_id'],how='left',copy=False)\n",
    "    data = pd.merge(data,gdp,on=['customer_id'],how='left',copy=False)\n",
    "    data = pd.merge(data,pay_deleta_time,on=['customer_id'],how='left',copy=False)\n",
    "    data = pd.merge(data,goods_list_time,on=['customer_id'],how='left',copy=False)\n",
    "    data = pd.merge(data,goods_num,on=['customer_id'],how='left',copy=False)\n",
    "    data = pd.merge(data,order_total_num,on=['customer_id'],how='left',copy=False)\n",
    "    data = pd.merge(data,order_total_payment_level,on=['customer_id'],how='left',copy=False)\n",
    "    data = pd.merge(data,customer_province_l,on=['customer_id'],how='left',copy=False)\n",
    "    data = pd.merge(data,last_time,on=['customer_id'],how='left',copy=False)\n",
    "    data['long_time'] = pd.to_datetime(data['order_pay_date_last']) - pd.to_datetime(data['order_pay_date_first'])\n",
    "    data['long_time'] = data['long_time'].dt.days + 1\n",
    "    del data['order_pay_date_first']\n",
    "    if isSubmit==False:\n",
    "        data['order_pay_date_last'] = pd.to_datetime(date2['order_pay_date'].min()) - pd.to_datetime(data['order_pay_date_last'])\n",
    "        data['order_pay_date_last'] = data['order_pay_date_last'].dt.days + 1\n",
    "        data['label'] = 0\n",
    "        data.loc[data['customer_id'].isin(list(date2['customer_id'].unique())),'label'] = 1\n",
    "        print(data['label'].mean())\n",
    "    else:\n",
    "        data['order_pay_date_last'] = pd.to_datetime('2013-12-31') - pd.to_datetime(data['order_pay_date_last'])\n",
    "        data['order_pay_date_last'] = data['order_pay_date_last'].dt.days + 1\n",
    "    print(data.shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = make_feature_and_label(train_history,train_label,False)\n",
    "submit = make_feature_and_label(online_history,None,True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
