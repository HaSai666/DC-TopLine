{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ksama\\AppData\\Local\\conda\\conda\\envs\\ml\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection  import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import datetime\n",
    "from tqdm import trange\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import warnings\n",
    "import time\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from numpy import *\n",
    "from sklearn.linear_model import  *\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "from xgboost import *\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.linear_model import  *\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sympy import *\n",
    "from xgboost import plot_importance\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "把结果保存成指定格式的文件\n",
    "示例：save_result2csv(y_submit,submit,'./round1_diac2019_test.csv')\n",
    "其中y_submit为模型输出结果，submit为读入数据时生成的一个变量，第三个参数为保存的文件路劲\n",
    "'''\n",
    "def save_result2csv(ys_submit,submit,csv_name):\n",
    "    all_customers = pd.DataFrame(submit[['customer_id']]).drop_duplicates(['customer_id']).dropna()\n",
    "    submits_df = submit[['customer_id']]\n",
    "    submits_df['result'] = ys_submit\n",
    "    all_customers = pd.merge(all_customers,submits_df,on=['customer_id'],how='left',copy=False)\n",
    "    all_customers = all_customers.sort_values(['customer_id'])\n",
    "    all_customers['customer_id'] = all_customers['customer_id'].astype('int64')\n",
    "    all_customers['result'] = all_customers['result'].fillna(0)\n",
    "    all_customers.to_csv(csv_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#官方的loss评估\n",
    "def logloss(y_true, y_pred,deta = 1.85, eps=1e-15):\n",
    "    # Prepare numpy array data\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    assert (len(y_true) and len(y_true) == len(y_pred))\n",
    "    # Clip y_pred between eps and 1-eps\n",
    "    p = np.clip(y_pred, eps, 1-eps)\n",
    "    loss = np.sum(- y_true * np.log(p) * deta - (1 - y_true) * np.log(1-p))\n",
    "    return loss / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查看结果的最大值，最小值，平均值\n",
    "def show(result):\n",
    "    result = np.array(result)\n",
    "    print('max: ',str(max(result)))\n",
    "    print('min: ',str(min(result)))\n",
    "    print('mean: ',str((sum(result))/(len(result))))\n",
    "    print('var: ',str(np.var(result)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = pd.read_csv('../Res/res_with11_final.csv',low_memory=False)\n",
    "res2 = pd.read_csv('../Res/res_without11_final.csv',low_memory=False)\n",
    "res3 = pd.read_csv('../Res/res_transfer_final.csv',low_memory=False)\n",
    "res4 = pd.read_csv('../Res/res_no11_transfer_final.csv',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"目前第三好 571\"\n",
    "res = pd.DataFrame()\n",
    "res[\"customer_id\"]=res1[\"customer_id\"]\n",
    "res[\"result\"] = (res2[\"result\"]+res3[\"result\"])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"目前第二好 568\"\n",
    "res = pd.DataFrame()\n",
    "res[\"customer_id\"]=res1[\"customer_id\"]\n",
    "res[\"result\"] = (res1[\"result\"]+res2[\"result\"]+res3[\"result\"]+res4[\"result\"])/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"目前最好 566\"\n",
    "res = pd.DataFrame()\n",
    "res[\"customer_id\"]=res1[\"customer_id\"]\n",
    "res[\"result\"] = (res3[\"result\"]+res4[\"result\"])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max:  0.9277857292636512\n",
      "min:  0.027892319161595455\n",
      "mean:  0.16952919975545075\n",
      "var:  0.019048288246949947\n",
      "\n",
      "max:  0.7846865393463149\n",
      "min:  0.07496140346608382\n",
      "mean:  0.2602056103758163\n",
      "var:  0.010601389836284917\n",
      "\n",
      "max:  0.8794963902470943\n",
      "min:  0.029285286603212737\n",
      "mean:  0.16919963339370553\n",
      "var:  0.018071965629011474\n",
      "\n",
      "max:  0.868436598415691\n",
      "min:  0.02961808305987087\n",
      "mean:  0.28298287287605556\n",
      "var:  0.019939322982813465\n",
      "\n",
      "max:  0.7996713400024472\n",
      "min:  0.029501768436060196\n",
      "mean:  0.22609125313488038\n",
      "var:  0.014137087108156782\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show(res1[\"result\"])\n",
    "show(res2[\"result\"])\n",
    "show(res3[\"result\"])\n",
    "show(res4[\"result\"])\n",
    "show(res[\"result\"])\n",
    "res.to_csv(\"../Res/res_final.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../Feature/ensemble_feature.csv',low_memory=False)\n",
    "y_train = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "[[218272    188]\n",
      " [ 54708    528]]\n",
      "Accuracy : 0.7994\n",
      "AUC Score (Train): 0.796117\n",
      "logloss SCore: 0.628610\n",
      "escape time: 4.665480375289917\n",
      "fold 1\n",
      "[[217938    174]\n",
      " [ 55091    493]]\n",
      "Accuracy : 0.7981\n",
      "AUC Score (Train): 0.794219\n",
      "logloss SCore: 0.632867\n",
      "escape time: 4.632637023925781\n",
      "fold 2\n",
      "[[218479    200]\n",
      " [ 54496    521]]\n",
      "Accuracy : 0.8002\n",
      "AUC Score (Train): 0.797162\n",
      "logloss SCore: 0.626883\n",
      "escape time: 4.6246044635772705\n",
      "fold 3\n",
      "[[218234    191]\n",
      " [ 54813    458]]\n",
      "Accuracy : 0.799\n",
      "AUC Score (Train): 0.794978\n",
      "logloss SCore: 0.629933\n",
      "escape time: 4.632611036300659\n",
      "fold 4\n",
      "[[218118    183]\n",
      " [ 54847    547]]\n",
      "Accuracy : 0.7989\n",
      "AUC Score (Train): 0.797550\n",
      "logloss SCore: 0.629440\n",
      "escape time: 4.682477235794067\n",
      "fold 5\n",
      "[[218377    198]\n",
      " [ 54571    550]]\n",
      "Accuracy : 0.7999\n",
      "AUC Score (Train): 0.794629\n",
      "logloss SCore: 0.628812\n",
      "escape time: 4.57676100730896\n",
      "fold 6\n",
      "[[217942    185]\n",
      " [ 55038    531]]\n",
      "Accuracy : 0.7982\n",
      "AUC Score (Train): 0.795776\n",
      "logloss SCore: 0.631756\n",
      "escape time: 4.593733549118042\n",
      "fold 7\n",
      "[[218252    187]\n",
      " [ 54722    535]]\n",
      "Accuracy : 0.7994\n",
      "AUC Score (Train): 0.796418\n",
      "logloss SCore: 0.628880\n",
      "escape time: 4.611637592315674\n",
      "fold 8\n",
      "[[218071    221]\n",
      " [ 54881    523]]\n",
      "Accuracy : 0.7987\n",
      "AUC Score (Train): 0.796466\n",
      "logloss SCore: 0.630276\n",
      "escape time: 4.535868883132935\n",
      "fold 9\n",
      "[[218360    184]\n",
      " [ 54631    520]]\n",
      "Accuracy : 0.7997\n",
      "AUC Score (Train): 0.797118\n",
      "logloss SCore: 0.627961\n",
      "escape time: 4.5578131675720215\n",
      "0.6295371372486711\n"
     ]
    }
   ],
   "source": [
    "# 将两次训练的lgb,xgb,rf的结果进行stacking\n",
    "#train_stack = np.vstack([y_pp_lgb_stacking,y_pp_lgb_stacking0,y_pp_xgb_stacking,y_pp_xgb_stacking0,y_pp_rf_stacking,y_pp_rf_stacking0]).transpose()\n",
    "#test_stack = np.vstack([y_pp_lgb,y_pp_lgb0,y_pp_xgb,y_pp_xgb0,y_pp_rf,y_pp_rf0]).transpose()\n",
    "\n",
    "#train_stack = np.vstack([y_pp_lgb_stacking,y_pp_lgb_stacking0,y_pp_xgb_stacking,y_pp_xgb_stacking0]).transpose()\n",
    "#test_stack = np.vstack([y_pp_lgb,y_pp_lgb0,y_pp_xgb,y_pp_xgb0]).transpose()\n",
    "train_stack = np.vstack([data['f1'],data['f2'],data['f3'],data['f4']]).transpose()\n",
    "test_stack = np.vstack([res2[\"result\"],res1[\"result\"],res3[\"result\"],res4[\"result\"]]).transpose()\n",
    "\n",
    "folds_stack = RepeatedKFold(n_splits=5, n_repeats=2, random_state=666)\n",
    "oof_stack = np.zeros(train_stack.shape[0])\n",
    "predictions = np.zeros(test_stack.shape[0])\n",
    "y_predall = np.zeros(train_stack.shape[0])\n",
    "y_predproball = np.zeros(train_stack.shape[0])\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack,y_train)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    t1 = time.time()\n",
    "    trn_data, trn_y = train_stack[trn_idx], y_train.iloc[trn_idx]\n",
    "    val_data, val_y = train_stack[val_idx], y_train.iloc[val_idx]\n",
    "\n",
    "\n",
    "    clf_5 =  lgb.LGBMClassifier(learning_rate =0.01,\n",
    "                                 n_estimators=100,\n",
    "                                 max_depth=8,\n",
    "                                 min_child_weight=1,\n",
    "                                 eta = 0.01,\n",
    "                                 alpha = 0.01,\n",
    "                                 gamma=0.0,\n",
    "                                 subsample=0.8,\n",
    "                                 colsample_bytree=0.8,\n",
    "                                 nthread=6,\n",
    "                                 scale_pos_weight=1,\n",
    "                                 seed=66)\n",
    "                             \n",
    "    #clf_5 = LinearDiscriminantAnalysis()\n",
    "    #clf_5 = LogisticRegression()\n",
    "    \n",
    "    clf_5.fit(trn_data, trn_y.astype(int))\n",
    "    \n",
    "    \n",
    "    y_pred = clf_5.predict(val_data) \n",
    "    y_predprob = clf_5.predict_proba(val_data)[:, 1] \n",
    "    y_predall[val_idx] = y_pred\n",
    "    y_predproball[val_idx] = y_predprob\n",
    "      \n",
    "    print(metrics.confusion_matrix(val_y, y_pred))\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(val_y, y_pred))  \n",
    "    auc = metrics.roc_auc_score(val_y, y_predprob)\n",
    "    print(\"AUC Score (Train): %f\" % auc) \n",
    "    loglossscore = logloss(val_y,y_predprob,deta=1.85)\n",
    "    print(\"logloss SCore: %f\" %loglossscore)\n",
    "    \n",
    "    \n",
    "    oof_stack[val_idx] = np.array(clf_5.predict_proba(val_data)[:,1])\n",
    "    predictions += np.array(clf_5.predict_proba(test_stack)[:,1])/ 10\n",
    "    t2 = time.time()\n",
    "    print('escape time:',str(t2-t1))\n",
    "print(logloss(y_train, oof_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../Feature/submit.csv',low_memory=False)\n",
    "save_result2csv(predictions,submit,'../Res/res_stacking.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max:  0.5372459912973524\n",
      "min:  0.07453150099817157\n",
      "mean:  0.25214444696620514\n",
      "var:  0.00954375572667282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
